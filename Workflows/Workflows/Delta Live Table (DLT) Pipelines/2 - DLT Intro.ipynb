{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fa623a7-b0bb-43dc-aa73-24ea2e171985",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# DLT works with 3 types of datasets\n",
    "# 1. Streaming tables (permanent/temporary) - Used as append data sources or for incremental data\n",
    "# 2. Materialized views - Used for transformations, aggregations or computations\n",
    "# 3. Views - Used for intermediate transformations, not stored in target schema\n",
    "\n",
    "import dlt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "505ed431-8cf4-4cc0-9f32-0d5881db67ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Rules for orders\n",
    "__order_rules = {\n",
    "    \"Valid Order Status\": \"o_orderstatus in ('O', 'F', 'P')\",\n",
    "    \"Valid Order Price\": \"o_totalprice > 0.0\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "197b6b79-2218-4529-b116-56fd68d716ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Rules for customers\n",
    "__customer_rules = {\n",
    "    \"Valid Customer Segment\": \"c_mktsegment is not null\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ac9495d-143b-40ef-bf4f-82b772c77c9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Streaming table for orders\n",
    "@dlt.table(\n",
    "    name=\"orders\",\n",
    "    table_properties={\n",
    "        \"quality\": \"bronze\"\n",
    "    },\n",
    "    comment=\"Raw orders data\"\n",
    ")\n",
    "@dlt.expect_all(__order_rules) # Default action is warn\n",
    "def orders_bronze(): # These functions have to return dataframes\n",
    "    df = (\n",
    "        spark\n",
    "        .readStream # For stream data\n",
    "        .table(\"dev.bronze.orders_raw\")\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53116573-74fc-4940-a4b3-d04ab32fcd81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Streaming table for orders using AutoLoader\n",
    "@dlt.table(\n",
    "    name=\"orders_autoloader\",\n",
    "    table_properties={\n",
    "        \"quality\": \"bronze\"\n",
    "    },\n",
    "    comment=\"Raw orders data autoloaded\"\n",
    ")\n",
    "@dlt.expect_all(__order_rules)\n",
    "def orders_autoloader_bronze(): # These functions have to return dataframes\n",
    "    df = (\n",
    "        spark\n",
    "        .readStream # For stream data\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.schemaHints\", \"o_orderkey long, o_custkey long, o_orderstatus string, o_totalprice decimal(18,2), o_orderdate date, o_orderpriority string, o_clerk string, o_shippriority integer, o_comment string\")\n",
    "        .option(\"cloudFiles.schemaLocation\", \"/Volumes/dev/etl/landing/autoloader/schema/1\")\n",
    "        .option(\"cloudFiles.format\", \"CSV\")\n",
    "        .option(\"pathGlobFilter\", \"*.csv\")\n",
    "        .option(\"cloudFiles.schemaEvolutionMode\", \"none\")\n",
    "        .load(\"/Volumes/dev/etl/landing/files/\")\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14ff637e-0d4b-4340-8f36-67550894bf54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Append Flow\n",
    "\n",
    "# Create target table\n",
    "dlt.create_streaming_table(\"orders_union_bronze\")\n",
    "\n",
    "@dlt.append_flow(\n",
    "    target=\"orders_union_bronze\"\n",
    ")\n",
    "def orders_bronze():\n",
    "    df = spark.readStream.table(\"LIVE.orders\") # Because this table is LIVE already in the pipeline\n",
    "    return df\n",
    "\n",
    "@dlt.append_flow(\n",
    "    target=\"orders_union_bronze\"\n",
    ")\n",
    "def orders_union():\n",
    "    df = spark.readStream.table(\"LIVE.orders_autoloader\") # Because this table is LIVE already in the pipeline\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e08339a5-69cf-4733-991f-ed7356a296a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Materialized view for customers\n",
    "@dlt.table(\n",
    "    name=\"customers\",\n",
    "    table_properties={\n",
    "        \"quality\": \"bronze\"\n",
    "    },\n",
    "    comment=\"Raw customers data\"\n",
    ")\n",
    "# @dlt.expect_all_or_fail(__customer_rules)\n",
    "@dlt.expect_all_or_drop(__customer_rules)\n",
    "def customers_bronze(): # These functions have to return dataframes\n",
    "    df = (\n",
    "        spark\n",
    "        .read # For batch data, always read in full (??)\n",
    "        # .readStream\n",
    "        .table(\"dev.bronze.customers_raw\")\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba47ea0d-3132-4cc0-b0a3-40127093f7d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# View to join orders with customers\n",
    "@dlt.view() # Used to create views\n",
    "def joined_view():\n",
    "    customers_df = spark.read.table(\"LIVE.customers\") # LIVE can be used to access DLTs produced in the same pipeline\n",
    "    # orders_df = spark.read.table(\"LIVE.orders\")\n",
    "    orders_df = spark.read.table(\"LIVE.orders_union_bronze\") # To read from the union table\n",
    "    joined_df = orders_df.join(customers_df, how=\"left_outer\", on=(orders_df[\"o_custkey\"] == customers_df[\"c_custkey\"]))\n",
    "\n",
    "    return joined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07f6b0d5-6d40-46f4-bf60-8f8d2008abf1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Materialized view with new column\n",
    "\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "@dlt.table(\n",
    "    table_properties = {\n",
    "        \"quality\": \"silver\"\n",
    "    },\n",
    "    name = \"orders_customers_joined_silver\",\n",
    "    comment = \"Orders joined with customers\"\n",
    ")\n",
    "def orders_customers_joined_silver():\n",
    "    joined_df = spark.read.table(\"LIVE.joined_view\") # Reading view\n",
    "    joined_df = joined_df.withColumn(\"__insert_date\", current_timestamp())\n",
    "    return joined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06f568e1-2a63-431c-9ec9-a12e00149c58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Aggregate based on c_mktsegment and find the count of order\n",
    "\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "@dlt.table(\n",
    "    table_properties = {\n",
    "        \"quality\": \"gold\"\n",
    "    },\n",
    "    name = \"orders_agg_mktsegment\",\n",
    "    comment = \"Orders aggregated based on market segment\"\n",
    ")\n",
    "def orders_agg_mktsegment():\n",
    "    agg_df = spark.read.table(\"LIVE.orders_customers_joined_silver\").groupBy(\"c_mktsegment\").agg(count(\"o_orderkey\").alias(\"num_orders\")).withColumn(\"__insert_date\", current_timestamp())\n",
    "    return agg_df"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2 - DLT Intro",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
